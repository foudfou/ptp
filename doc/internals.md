# Internals

## Kademlia

`net/kad/routes.c`

> In Kademlia, peers are virtually structured as leaves of a binary tree,
> which can also be vizualized as a ring. Peers are placed in the tree by
> their node ID, which is a N bits number.

Distance(A, B) = A XOR B.

The node ID `kad_guid` (`net/kad/id.h`) is a byte array of k = 20 bytes (1 in
tests, 8 in libtorrent). Kad parameters are defined in `net/kad/defs.h.in`.

> The **routing table** is implemented as hash table[^1]: an array of lists
> (buckets) of at most KAD\_K_CONST node entries [k = 8]. Instead of using a
> generic hash table implementation, we build a specialized one for specific
> operations on each list. Lists are sorted by design: depending on if we want
> to evict nodes (buckets) or get the most recent ones (replacements).

`struct list_item buckets[KAD_GUID_SPACE_IN_BITS]` (`net/kad/routes.h`).

Each **bucket** (aka *k-bucket*) points to a list of `struct kad_node`, linked
via an inner `struct list_item` (`utils/list.h`):

> A Linux-style circular list where the data *contains* the list.

Each bucket holds up to k active nodes (least-recently seen at the head). When
a bucket is full, a new node is placed into the **replacement cache**.

> The next time the node queries contacts in the k-bucket, any unresponsive
> ones can be evicted and replaced with entries in the replacement cache.

A **node** (`struct kad_node`) refers to a kademlia node, which we
differentiate from a **peer** (`struct peer` in `net/actions.h`):

> A "peer" is a client/server listening on a TCP port that implements some
> specific protocol (msg). A "node" is a client/server listening on a UDP port
> implementing the distributed hash table protocol (kad). [^2]

A given bucket receives nodes that are within a distance of 2^i..2^i+1 with the
current node. Which is to say that a buckets receives nodes for which the
distance with self share the same prefix.

> Ex: In a 4 bits space,for node 0 and k=3,
>   bucket 0 has nodes of distance 0..2 = node 000x, actually only 0001
>   bucket 1 has nodes of distance 2..4 = nodes 001x
>   bucket 2 has nodes of distance 4..8 = nodes 01xx
>   bucket 3 has nodes of distance 8..16 = nodes 1xxx
> Each bucket will hold up to k active nodes.

## Event loop

`server_run()` (`server.c`) creates 2 sockets, TCP and UDP, and `poll(s)`s on
them.

The loop handles events. Events are created in 2 ways: *timers* (`timers.h`) or
*events* directly. The *event queue* (`evq`) holds events to dispatch. Timers
eventually push their events to the queue (`timers_apply()`). I.e. on each run,
the loop: 1. applies timers, 2. dispatches events from the queue.

I/O events generated by the `poll()` push events (`event_queue_put()`) to the
queue.

Timers are created via:

- `timer_init()`, which accepts a `struct timer` and can repeat;
- `set_timeout()`. which accepts an event type and creates a one-time timer
  (`.once=true`).

Timer events are effectively dispatched *after* their `.delay`
A timer holds an event type, ex: `event_kad_refresh`, with possible arguments.
The event type points to a callback, ex: `event_kad_refresh_cb()`. All
callbacks have the same signature, but point to the actual callback function,
ex: `kad_refresh()`. Server callback functions are grouped as *actions*
(`net/actions.c`).

By convention, we set the `.self` field of (m)allocated events and timers, so
they can be freed later. `.self` should be null if allocated on the stack.

## Protocols

### Dummy (TCP)

The TCP protocol is mostly an empty shell (2025-08). See `event_peer_data_cb` (`events.c`),
`peer_conn_handle_data()` (`net/actions.c`).

> Peers exchange messages in the Type-Length-Data (tlv) format.

### Kad RPC (UDP)

The server reacts to UDP datagrams:

```
event_node_data > node_handle_data() > kad_rpc_handle
```
Messages are encoded in [Bencode](https://www.bittorrent.org/beps/bep_0003.html).

> The Kademlia protocol consists of four RPCs: PING, STORE, FIND\_NODE, and
> FIND_VALUE.

#### Bencode

```
benc_decode_rpc_msg(&msg, buf, strlen(buf) ⇒ generates kad msg from buf
benc_parse(&repr, buf, slen) ⇒ actual parsing, ends up calling…
benc_repr_build(repr, &parser, &lit, tok)) ⇒ creates representation
```

* string: `<length>:<contents>`. Ex: `4:spam`.
* integer: `i<number>e`. Ex: `i3e`.
* list: `l<elements>e`. Ex: `l4:spami3ee` (`['spam',3]`).
* dictionary: `d<contents>e`. Ex: `d4:spaml1:a1:bee` (`{'spam':['a','b']}`).

> Keys must be strings and appear in sorted order.


Messages are dictionaries[^2].

```
"t" transaction id: 2 chars.
"y" message type: "q" for query, "r" for response, or "e" for error.

"q" query method name: str.
"a" named arguments: dict.

"r" named return values: dict.

"e" list: error code (int), error msg (str).
```

> Parsing consists in building a representation of the bencode object. This is
> done via 2 data structures: a tree of nodes and a list of literal values for
> str|int. Nodes can be of type: dict|dict_entry|list|literal. In practice
> nodes are stored into an array. Each node points to other nodes.

##### Compact node info

Kad node info is serialized as a string comprising IP address + port. For
certain responses, noticeably `FIND_NODE`, [**our implementation diverges from
the standard BitTorrent
implementation**](https://github.com/arvidn/libtorrent/blob/e2b12e72d89d3037a4d927bef70d663b1fbb2530/src/kademlia/node.cpp#L759):
BitTorrent uses a single string for multiple nodes where we use a list.

##### Parser Types

There a different parser types, based on use cases, each with a different allocation strategy:

- **`BENC_PARSER_GLOBAL`**: large static buffers (~3.2MB) for routing table serialization and bootstrap data
- **`BENC_PARSER_NET`**: small stack buffers (~3KB) for network messages

The parser validates syntax during traversal, ensuring malformed bencode
triggers immediate failure regardless of allocation strategy.

#### Bootstrap

When the server starts, it schedules 2 actions:

- bootstrap `event_kad_bootstrap`, once and immediately
  - insert boostrap nodes read from files into routing table
  - lookup self
- refresh `event_kad_refresh_bootstrap`, once and immediately: refresh all
  stale k-buckets further away than its closest neighbor.
  TODO not-implemented-yet.

#### Refresh

See `event_kad_refresh`, every 5 minutes. TODO not-implemented-yet.

> Refreshing means picking a random ID in the bucket’s range and performing a
> node search for that ID.
>
> Each node refreshes any bucket to which it has not performed a node lookup in
> the past hour.

#### Resource store

**This is not covered yet** (2025-08)

Kademlia operates as a distributed key-value store with two distinct
types of keys:

- Node IDs for participating nodes in the network
- Resource keys: identifiers for data/resources being stored

Both use the same key space (typically 160-bit SHA-1 hashes), but serve
different purposes.

Node lookup is handled via the routing table and covered in a later
paragraph. But resource location is based on the principle that **a resource
with key K is stored on nodes "close" to node ID = K**.

To store/find a resource with key K:

1. use `FIND_NODE(K)` to locate the nodes with IDs closest to K
2. then use `STORE` to place the value on those closest nodes, or
   `FIND_VALUE(K)` to retrieve it

For example, bittorrent stores: key = info_hash, value = list of peers for that
torrent. A node typically stores many key-value pairs.

## Kad lookup

FIXME

[^1]: The routing table is usually implemented in 2 flavors: a fixed-sized hash
    table, where k-buckets represent the *distance* prefix; or a tree, where
    tree nodes are *node ID* prefixes / "bit splits" (ex: `1xxx` → left:`10xx`,
    right:`11xx`) leaf nodes buckets, i.e. a list of kad nodes. In this
    version, buckets represent regions of the network space, a kind of node ID
    *trie*.

[^2]: BitTorrent's [DHT Protocol](https://bittorrent.org/beps/bep_0005.html).
